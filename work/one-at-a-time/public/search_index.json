[
["index.html", "One at a time Prerequisites", " One at a time Yihui Xie 2019-09-20 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["working-with-data-frame.html", "Chapter 1 Working with data.frame 1.1 Load PyTorch libraries 1.2 Dataset iteration batch settings 1.3 Summary statistics for tensors 1.4 using data.frame", " Chapter 1 Working with data.frame 1.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) 1.2 Dataset iteration batch settings # folders where the images are located train_data_path = &#39;~/mnist_png_full/training/&#39; test_data_path = &#39;~/mnist_png_full/testing/&#39; # read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: /home/msfz751/mnist_png_full/training/ 1.3 Summary statistics for tensors 1.4 using data.frame library(tictoc) tic() fun_list &lt;- list( size = c(&quot;size&quot;), numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 599L) # how many samples fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[1]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return chracater eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } df &lt;- data.frame(ridx = idx+1, # index number for the sample do.call(data.frame, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) head(df) #&gt; ridx size numel sum mean std med max min #&gt; 1 1 torch.Size([3, 28, 28]) 2352 366 0.156 0.329 0 1 0 #&gt; 2 2 torch.Size([3, 28, 28]) 2352 284 0.121 0.297 0 1 0 #&gt; 3 3 torch.Size([3, 28, 28]) 2352 645 0.274 0.420 0 1 0 #&gt; 4 4 torch.Size([3, 28, 28]) 2352 410 0.174 0.355 0 1 0 #&gt; 5 5 torch.Size([3, 28, 28]) 2352 321 0.137 0.312 0 1 0 #&gt; 6 6 torch.Size([3, 28, 28]) 2352 654 0.278 0.421 0 1 0 toc() #&gt; 12.697 sec elapsed # 59 1.663s # 599 13.5s # 5999 54.321 sec; 137.6s # 59999 553.489 sec elapsed "],
["working-with-data-table.html", "Chapter 2 Working with data.table 2.1 Load PyTorch libraries", " Chapter 2 Working with data.table 2.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) ## Dataset iteration batch settings # folders where the images are located train_data_path = &#39;~/mnist_png_full/training/&#39; test_data_path = &#39;~/mnist_png_full/testing/&#39; # read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: /home/msfz751/mnist_png_full/training/ 2.1.1 Using `data.table library(data.table) library(tictoc) tic() fun_list &lt;- list( numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 5999L) fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[1]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return chracater eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } dt &lt;- data.table(ridx = idx+1, do.call(data.table, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) head(dt) #&gt; ridx numel sum mean std med max min #&gt; 1: 1 2352 366 0.156 0.329 0 1 0 #&gt; 2: 2 2352 284 0.121 0.297 0 1 0 #&gt; 3: 3 2352 645 0.274 0.420 0 1 0 #&gt; 4: 4 2352 410 0.174 0.355 0 1 0 #&gt; 5: 5 2352 321 0.137 0.312 0 1 0 #&gt; 6: 6 2352 654 0.278 0.421 0 1 0 toc() #&gt; 115.585 sec elapsed # 60 1.266 sec elapsed # 600 11.798 sec elapsed; 12.9s # 6000 119.256 sec elapsed; 132.9s # 1117.619 sec elapsed "],
["simple-regression-with-pytorch.html", "Chapter 3 Simple Regression with PyTorch 3.1 Creating the network model 3.2 Datasets 3.3 Optimizer and Loss 3.4 Training 3.5 Result", " Chapter 3 Simple Regression with PyTorch This examples combine Python and R code together. Source: https://www.guru99.com/pytorch-tutorial.html 3.1 Creating the network model Our network model is a simple Linear layer with an input and an output shape of 1. library(rTorch) from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable torch.manual_seed(123) #&gt; &lt;torch._C.Generator object at 0x7f59ffafca90&gt; class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x net = Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) And the network output should be like this Net( (hidden): Linear(in_features=1, out_features=1, bias=True) ) 3.1.1 Code in R This would be the equivalent code in R: library(reticulate) #&gt; #&gt; Attaching package: &#39;reticulate&#39; #&gt; The following objects are masked from &#39;package:rTorch&#39;: #&gt; #&gt; conda_install, conda_python torch &lt;- import(&quot;torch&quot;) nn &lt;- import(&quot;torch.nn&quot;) Variable &lt;- import(&quot;torch.autograd&quot;)$Variable torch$manual_seed(123) #&gt; &lt;torch._C.Generator&gt; main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) 3.2 Datasets Before you start the training process, you need to know our data. You make a random function to test our model. \\(Y = x3 sin(x)+ 3x+0.8 rand(100)\\) # Visualize our data import matplotlib.pyplot as plt import numpy as np np.random.seed(123) x = np.random.rand(100) y = np.sin(x) * np.power(x,3) + 3*x + np.random.rand(100)*0.8 plt.scatter(x, y) plt.show() This is the code in R: np &lt;- import(&quot;numpy&quot;) np$random$seed(123L) x = np$random$rand(100L) y = np$sin(x) * np$power(x, 3L) + 3*x + np$random$rand(100L)*0.8 plot(x, y) Before you start the training process, you need to convert the numpy array to Variables that supported by Torch and autograd. # convert numpy array to tensor in shape of input size x = torch.from_numpy(x.reshape(-1,1)).float() y = torch.from_numpy(y.reshape(-1,1)).float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]]) tensor([[2.7166], #&gt; [1.3983], #&gt; [0.7679], #&gt; [1.8464], #&gt; [2.6614], #&gt; [1.8297], #&gt; [4.4034], #&gt; [2.7003], #&gt; [2.1778], #&gt; [1.5073], #&gt; [1.2966], #&gt; [2.7287], #&gt; [1.4884], #&gt; [0.8423], #&gt; [1.4895], #&gt; [2.9263], #&gt; [1.0114], #&gt; [0.9445], #&gt; [1.6729], #&gt; [2.4624], #&gt; [2.7788], #&gt; [3.1746], #&gt; [2.6593], #&gt; [2.3800], #&gt; [3.1382], #&gt; [1.7665], #&gt; [1.3082], #&gt; [1.1390], #&gt; [1.5341], #&gt; [2.3566], #&gt; [0.8612], #&gt; [1.4642], #&gt; [1.8066], #&gt; [2.2308], #&gt; [2.0962], #&gt; [1.0096], #&gt; [1.6538], #&gt; [3.3994], #&gt; [3.8747], #&gt; [2.0045], #&gt; [2.0884], #&gt; [0.5845], #&gt; [1.7039], #&gt; [1.7285], #&gt; [3.4602], #&gt; [1.3581], #&gt; [2.0949], #&gt; [3.7935], #&gt; [2.1950], #&gt; [2.6425], #&gt; [0.4948], #&gt; [3.5188], #&gt; [2.1628], #&gt; [1.9643], #&gt; [1.5740], #&gt; [1.0099], #&gt; [1.8123], #&gt; [2.9534], #&gt; [3.6986], #&gt; [1.9485], #&gt; [2.5445], #&gt; [2.4811], #&gt; [2.4700], #&gt; [2.2838], #&gt; [3.4392], #&gt; [0.9015], #&gt; [2.8687], #&gt; [1.4766], #&gt; [1.1847], #&gt; [2.2782], #&gt; [0.8885], #&gt; [3.2565], #&gt; [2.7141], #&gt; [3.0781], #&gt; [0.7763], #&gt; [2.0038], #&gt; [1.8270], #&gt; [0.5882], #&gt; [0.7793], #&gt; [2.6416], #&gt; [1.4162], #&gt; [2.3851], #&gt; [1.9140], #&gt; [1.8385], #&gt; [3.7822], #&gt; [3.6160], #&gt; [1.0941], #&gt; [0.5721], #&gt; [1.6683], #&gt; [1.6848], #&gt; [2.5068], #&gt; [4.3876], #&gt; [1.3866], #&gt; [3.1286], #&gt; [1.9197], #&gt; [2.7949], #&gt; [0.4797], #&gt; [1.8171], #&gt; [1.1042], #&gt; [1.1414]]) 3.2.1 Code in R Notice that before converting to a Torch tensor, we need first to convert the R numeric vector to a numpy array: # convert numpy array to tensor in shape of input size x &lt;- r_to_py(x) y &lt;- r_to_py(y) x = torch$from_numpy(x$reshape(-1L, 1L)) #$float() y = torch$from_numpy(y$reshape(-1L, 1L)) #$float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]], dtype=torch.float64) 3.3 Optimizer and Loss Next, you should define the Optimizer and the Loss Function for our training process. # Define Optimizer and Loss Function optimizer = torch.optim.SGD(net.parameters(), lr=0.2) loss_func = torch.nn.MSELoss() print(optimizer) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) print(loss_func) #&gt; MSELoss() 3.3.1 Equivalent code in R # Define Optimizer and Loss Function optimizer &lt;- torch$optim$SGD(net$parameters(), lr=0.2) loss_func &lt;- torch$nn$MSELoss() print(optimizer) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) print(loss_func) #&gt; MSELoss() 3.4 Training 3.4.1 Code in Python Now let’s start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters. inputs = Variable(x) outputs = Variable(y) for i in range(250): prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer.zero_grad() loss.backward() optimizer.step() if i % 10 == 0: # plot and show learning process plt.cla() plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=2) plt.text(0.5, 0, &#39;Loss=%.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 10, &#39;color&#39;: &#39;red&#39;}) plt.pause(0.1) plt.show() 3.4.2 Code in R x = x$type(torch$FloatTensor) # make it a a FloatTensor y = y$type(torch$FloatTensor) inputs = Variable(x) outputs = Variable(y) plot(x$data$numpy(), y$data$numpy(), col = &quot;blue&quot;) for (i in 1:250) { prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer$zero_grad() loss$backward() optimizer$step() if (i %% 10 == 0) { # plot and show learning process # points(x$data$numpy(), y$data$numpy()) points(x$data$numpy(), prediction$data$numpy(), col=&quot;red&quot;) # cat(i, loss$data$numpy(), &quot;\\n&quot;) } } 3.5 Result As you can see below, you successfully performed regression with a neural network. Actually, on every iteration, the red line in the plot will update and change its position to fit the data. But in this picture, you only show you the final result "],
["autograd.html", "Chapter 4 Autograd 4.1 Python code 4.2 R code 4.3 Observations", " Chapter 4 Autograd Source: https://github.com/jcjohnson/pytorch-examples#pytorch-autograd library(rTorch) 4.1 Python code # Do not print from a function. Similar functionality to R invisible() # https://stackoverflow.com/a/45669280/5270873 import os, sys class HiddenPrints: def __enter__(self): self._original_stdout = sys.stdout sys.stdout = open(os.devnull, &#39;w&#39;) def __exit__(self, exc_type, exc_val, exc_tb): sys.stdout.close() sys.stdout = self._original_stdout # Code in file autograd/two_layer_net_autograd.py import torch device = torch.device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU torch.manual_seed(0) # N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension. #&gt; &lt;torch._C.Generator object at 0x7f4c2787ab90&gt; N, D_in, H, D_out = 64, 1000, 100, 10 # Create random Tensors to hold input and outputs x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # Create random Tensors for weights; setting requires_grad=True means that we # want to compute gradients for these Tensors during the backward pass. w1 = torch.randn(D_in, H, device=device, requires_grad=True) w2 = torch.randn(H, D_out, device=device, requires_grad=True) learning_rate = 1e-6 for t in range(5): # Forward pass: compute predicted y using operations on Tensors. Since w1 and # w2 have requires_grad=True, operations involving these Tensors will cause # PyTorch to build a computational graph, allowing automatic computation of # gradients. Since we are no longer implementing the backward pass by hand we # don&#39;t need to keep references to intermediate values. y_pred = x.mm(w1).clamp(min=0).mm(w2) # Compute and print loss. Loss is a Tensor of shape (), and loss.item() # is a Python number giving its value. loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call w1.grad and w2.grad will be Tensors holding the gradient # of the loss with respect to w1 and w2 respectively. loss.backward() # Update weights using gradient descent. For this step we just want to mutate # the values of w1 and w2 in-place; we don&#39;t want to build up a computational # graph for the update steps, so we use the torch.no_grad() context manager # to prevent PyTorch from building a computational graph for the updates with torch.no_grad(): w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # Manually zero the gradients after running the backward pass with HiddenPrints(): # this would be the equivalent of invisible() in R w1.grad.zero_() w2.grad.zero_() #&gt; 0 29428666.0 #&gt; 1 22739450.0 #&gt; 2 20605262.0 #&gt; 3 19520376.0 #&gt; 4 17810228.0 4.2 R code # library(reticulate) # originally qwe used reticulate library(rTorch) torch = import(&quot;torch&quot;) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; # N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension. N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x = torch$randn(N, D_in, device=device) y = torch$randn(N, D_out, device=device) # Create random Tensors for weights; setting requires_grad=True means that we # want to compute gradients for these Tensors during the backward pass. w1 = torch$randn(D_in, H, device=device, requires_grad=TRUE) w2 = torch$randn(H, D_out, device=device, requires_grad=TRUE) learning_rate = torch$scalar_tensor(1e-6) for (t in 1:5) { # Forward pass: compute predicted y using operations on Tensors. Since w1 and # w2 have requires_grad=True, operations involving these Tensors will cause # PyTorch to build a computational graph, allowing automatic computation of # gradients. Since we are no longer implementing the backward pass by hand we # don&#39;t need to keep references to intermediate values. y_pred = x$mm(w1)$clamp(min=0)$mm(w2) # Compute and print loss. Loss is a Tensor of shape (), and loss.item() # is a Python number giving its value. loss = (torch$sub(y_pred, y))$pow(2)$sum() cat(t, &quot;\\t&quot;, loss$item(), &quot;\\n&quot;) # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call w1.grad and w2.grad will be Tensors holding the gradient # of the loss with respect to w1 and w2 respectively. loss$backward() # Update weights using gradient descent. For this step we just want to mutate # the values of w1 and w2 in-place; we don&#39;t want to build up a computational # graph for the update steps, so we use the torch.no_grad() context manager # to prevent PyTorch from building a computational graph for the updates with(torch$no_grad(), { w1$data = torch$sub(w1$data, torch$mul(w1$grad, learning_rate)) w2$data = torch$sub(w2$data, torch$mul(w2$grad, learning_rate)) # Manually zero the gradients after running the backward pass w1$grad$zero_() w2$grad$zero_() }) } #&gt; 1 29428666 #&gt; 2 22739450 #&gt; 3 20605262 #&gt; 4 19520376 #&gt; 5 17810228 4.3 Observations If the seeds worked the same in Python and R, we should see similar results in the output. "]
]
