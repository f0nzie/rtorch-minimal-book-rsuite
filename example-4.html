<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Example | A Minimal rTorch Tutorial</title>
  <meta name="description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Example | A Minimal rTorch Tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Example | A Minimal rTorch Tutorial" />
  
  <meta name="twitter:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2019-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rainfall-linear-regression.html"/>
<link rel="next" href="a-very-simple-neural-network.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal rTorch Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-anaconda"><i class="fa fa-check"></i>Python Anaconda</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#automatic-installation"><i class="fa fa-check"></i>Automatic installation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Getting Started</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>1.2</b> How do we start using <code>rTorch</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#getting-the-pytorch-version"><i class="fa fa-check"></i><b>1.2.1</b> Getting the PyTorch version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#pytorch-configuration"><i class="fa fa-check"></i><b>1.2.2</b> PyTorch configuration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>1.3</b> What can you do with <code>rTorch</code></a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#the-torchvision-module"><i class="fa fa-check"></i><b>1.3.1</b> The <code>torchvision</code> module</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#np-the-numpy-module"><i class="fa fa-check"></i><b>1.3.2</b> <code>np</code>: the <code>numpy</code> module</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#python-built-in-functions"><i class="fa fa-check"></i><b>1.3.3</b> Python built-in functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html"><i class="fa fa-check"></i><b>2</b> rTorch vs PyTorch: What’s different</a><ul>
<li class="chapter" data-level="2.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#calling-objects-from-pytorch"><i class="fa fa-check"></i><b>2.1</b> Calling objects from PyTorch</a></li>
<li class="chapter" data-level="2.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#call-a-module-from-pytorch"><i class="fa fa-check"></i><b>2.2</b> Call a module from PyTorch</a></li>
<li class="chapter" data-level="2.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#show-the-attributes-methods-of-a-class-or-pytorch-object"><i class="fa fa-check"></i><b>2.3</b> Show the attributes (methods) of a class or PyTorch object</a></li>
<li class="chapter" data-level="2.4" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#enumeration"><i class="fa fa-check"></i><b>2.4</b> Enumeration</a></li>
<li class="chapter" data-level="2.5" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#how-to-iterate"><i class="fa fa-check"></i><b>2.5</b> How to iterate</a><ul>
<li class="chapter" data-level="2.5.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>2.5.1</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="2.5.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-a-for-loop-to-iterate"><i class="fa fa-check"></i><b>2.5.2</b> Using a <code>for-loop</code> to iterate</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#zero-gradient"><i class="fa fa-check"></i><b>2.6</b> Zero gradient</a><ul>
<li class="chapter" data-level="2.6.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-python"><i class="fa fa-check"></i><b>2.6.1</b> Version in Python</a></li>
<li class="chapter" data-level="2.6.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-r"><i class="fa fa-check"></i><b>2.6.2</b> Version in R</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#transform-a-tensor"><i class="fa fa-check"></i><b>2.7</b> Transform a tensor</a></li>
<li class="chapter" data-level="2.8" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#build-a-model-class"><i class="fa fa-check"></i><b>2.8</b> Build a model class</a><ul>
<li class="chapter" data-level="2.8.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-1"><i class="fa fa-check"></i><b>2.8.1</b> Example 1</a></li>
<li class="chapter" data-level="2.8.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-2"><i class="fa fa-check"></i><b>2.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>2.9</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="2.10" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>2.10</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="part"><span><b>II Basic Tensor Operations</b></span></li>
<li class="chapter" data-level="3" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>3</b> Tensors</a><ul>
<li class="chapter" data-level="3.1" data-path="tensors.html"><a href="tensors.html#arithmetic-of-tensors"><i class="fa fa-check"></i><b>3.1</b> Arithmetic of tensors</a></li>
<li class="chapter" data-level="3.2" data-path="tensors.html"><a href="tensors.html#boolean-operations"><i class="fa fa-check"></i><b>3.2</b> Boolean operations</a></li>
<li class="chapter" data-level="3.3" data-path="tensors.html"><a href="tensors.html#slicing"><i class="fa fa-check"></i><b>3.3</b> Slicing</a></li>
<li class="chapter" data-level="3.4" data-path="tensors.html"><a href="tensors.html#example-3"><i class="fa fa-check"></i><b>3.4</b> Example</a><ul>
<li class="chapter" data-level="3.4.1" data-path="tensors.html"><a href="tensors.html#load-the-libraries"><i class="fa fa-check"></i><b>3.4.1</b> Load the libraries</a></li>
<li class="chapter" data-level="3.4.2" data-path="tensors.html"><a href="tensors.html#datasets"><i class="fa fa-check"></i><b>3.4.2</b> Datasets</a></li>
<li class="chapter" data-level="3.4.3" data-path="tensors.html"><a href="tensors.html#run-the-model"><i class="fa fa-check"></i><b>3.4.3</b> Run the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra with Torch</a><ul>
<li class="chapter" data-level="4.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scalars"><i class="fa fa-check"></i><b>4.1</b> Scalars</a></li>
<li class="chapter" data-level="4.2" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors"><i class="fa fa-check"></i><b>4.2</b> Vectors</a></li>
<li class="chapter" data-level="4.3" data-path="linearalgebra.html"><a href="linearalgebra.html#matrices"><i class="fa fa-check"></i><b>4.3</b> Matrices</a></li>
<li class="chapter" data-level="4.4" data-path="linearalgebra.html"><a href="linearalgebra.html#d-tensors"><i class="fa fa-check"></i><b>4.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="4.5" data-path="linearalgebra.html"><a href="linearalgebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>4.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="4.6" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>4.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="4.7" data-path="linearalgebra.html"><a href="linearalgebra.html#tensor-arithmetic"><i class="fa fa-check"></i><b>4.7</b> Tensor arithmetic</a></li>
<li class="chapter" data-level="4.8" data-path="linearalgebra.html"><a href="linearalgebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>4.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="4.9" data-path="linearalgebra.html"><a href="linearalgebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>4.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="4.10" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>4.10</b> Dot product</a></li>
</ul></li>
<li class="part"><span><b>III Logistic Regression</b></span></li>
<li class="chapter" data-level="5" data-path="mnistdigits.html"><a href="mnistdigits.html"><i class="fa fa-check"></i><b>5</b> Example 1: MNIST handwritten digits</a><ul>
<li class="chapter" data-level="5.1" data-path="mnistdigits.html"><a href="mnistdigits.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="5.2" data-path="mnistdigits.html"><a href="mnistdigits.html#read-datasets"><i class="fa fa-check"></i><b>5.2</b> Read datasets</a></li>
<li class="chapter" data-level="5.3" data-path="mnistdigits.html"><a href="mnistdigits.html#define-the-model"><i class="fa fa-check"></i><b>5.3</b> Define the model</a></li>
<li class="chapter" data-level="5.4" data-path="mnistdigits.html"><a href="mnistdigits.html#training"><i class="fa fa-check"></i><b>5.4</b> Training</a></li>
<li class="chapter" data-level="5.5" data-path="mnistdigits.html"><a href="mnistdigits.html#prediction"><i class="fa fa-check"></i><b>5.5</b> Prediction</a></li>
<li class="chapter" data-level="5.6" data-path="mnistdigits.html"><a href="mnistdigits.html#save-the-model"><i class="fa fa-check"></i><b>5.6</b> Save the model</a></li>
</ul></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Simple linear regression</a><ul>
<li class="chapter" data-level="6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#generate-the-dataset"><i class="fa fa-check"></i><b>6.2</b> Generate the dataset</a></li>
<li class="chapter" data-level="6.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#convert-arrays-to-tensors"><i class="fa fa-check"></i><b>6.3</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="6.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>6.4</b> Converting from numpy to tensor</a></li>
<li class="chapter" data-level="6.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>6.5</b> Creating the network model</a></li>
<li class="chapter" data-level="6.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>6.6</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="6.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#training-1"><i class="fa fa-check"></i><b>6.7</b> Training</a></li>
<li class="chapter" data-level="6.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#results"><i class="fa fa-check"></i><b>6.8</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html"><i class="fa fa-check"></i><b>7</b> Rainfall. Linear Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#training-data"><i class="fa fa-check"></i><b>7.1</b> Training data</a></li>
<li class="chapter" data-level="7.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#convert-arrays-to-tensors-1"><i class="fa fa-check"></i><b>7.2</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="7.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>7.3</b> Build the model</a></li>
<li class="chapter" data-level="7.4" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>7.4</b> Generate predictions</a></li>
<li class="chapter" data-level="7.5" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#loss-function"><i class="fa fa-check"></i><b>7.5</b> Loss Function</a></li>
<li class="chapter" data-level="7.6" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#step-by-step-process"><i class="fa fa-check"></i><b>7.6</b> Step by step process</a><ul>
<li class="chapter" data-level="7.6.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-the-losses"><i class="fa fa-check"></i><b>7.6.1</b> Compute the losses</a></li>
<li class="chapter" data-level="7.6.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>7.6.2</b> Compute Gradients</a></li>
<li class="chapter" data-level="7.6.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#reset-the-gradients"><i class="fa fa-check"></i><b>7.6.3</b> Reset the gradients</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#all-together-train-for-multiple-epochs"><i class="fa fa-check"></i><b>7.7</b> All together: train for multiple epochs</a></li>
</ul></li>
<li class="part"><span><b>V Neural Networks</b></span></li>
<li class="chapter" data-level="8" data-path="example-4.html"><a href="example-4.html"><i class="fa fa-check"></i><b>8</b> Example</a><ul>
<li class="chapter" data-level="8.1" data-path="example-4.html"><a href="example-4.html#load-the-libraries-1"><i class="fa fa-check"></i><b>8.1</b> Load the libraries</a></li>
<li class="chapter" data-level="8.2" data-path="example-4.html"><a href="example-4.html#dataset"><i class="fa fa-check"></i><b>8.2</b> Dataset</a></li>
<li class="chapter" data-level="8.3" data-path="example-4.html"><a href="example-4.html#run-the-model-for-50-iterations"><i class="fa fa-check"></i><b>8.3</b> Run the model for 50 iterations</a></li>
<li class="chapter" data-level="8.4" data-path="example-4.html"><a href="example-4.html#run-it-at-100-iterations"><i class="fa fa-check"></i><b>8.4</b> Run it at 100 iterations</a></li>
<li class="chapter" data-level="8.5" data-path="example-4.html"><a href="example-4.html#original-pytorch-code"><i class="fa fa-check"></i><b>8.5</b> Original PyTorch code</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html"><i class="fa fa-check"></i><b>9</b> A very simple neural network</a><ul>
<li class="chapter" data-level="9.1" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#select-device"><i class="fa fa-check"></i><b>9.2</b> Select device</a></li>
<li class="chapter" data-level="9.3" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#create-the-dataset"><i class="fa fa-check"></i><b>9.3</b> Create the dataset</a></li>
<li class="chapter" data-level="9.4" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#define-the-model-1"><i class="fa fa-check"></i><b>9.4</b> Define the model</a></li>
<li class="chapter" data-level="9.5" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#loss-function-1"><i class="fa fa-check"></i><b>9.5</b> Loss function</a></li>
<li class="chapter" data-level="9.6" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#iterate-through-batches"><i class="fa fa-check"></i><b>9.6</b> Iterate through batches</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="neural-networks-2.html"><a href="neural-networks-2.html"><i class="fa fa-check"></i><b>10</b> Neural Networks 2</a><ul>
<li class="chapter" data-level="10.1" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-1"><i class="fa fa-check"></i><b>10.1</b> nn2 1</a></li>
<li class="chapter" data-level="10.2" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-2"><i class="fa fa-check"></i><b>10.2</b> nn2 2</a></li>
</ul></li>
<li class="part"><span><b>VI Image Recognition</b></span></li>
<li class="part"><span><b>VII PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="11" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>11</b> Working with data.frame</a><ul>
<li class="chapter" data-level="11.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>11.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="11.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#dataset-iteration-batch-settings"><i class="fa fa-check"></i><b>11.2</b> Dataset iteration batch settings</a></li>
<li class="chapter" data-level="11.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>11.3</b> Summary statistics for tensors</a></li>
<li class="chapter" data-level="11.4" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>11.4</b> using <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>12</b> Working with data.table</a><ul>
<li class="chapter" data-level="12.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>12.1</b> Load PyTorch libraries</a><ul>
<li class="chapter" data-level="12.1.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>12.1.1</b> Using `data.table</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixA.html"><a href="appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixA.html"><a href="appendixA.html#basic-statistical-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a><ul>
<li class="chapter" data-level="A.1.1" data-path="appendixA.html"><a href="appendixA.html#mean"><i class="fa fa-check"></i><b>A.1.1</b> Mean</a></li>
<li class="chapter" data-level="A.1.2" data-path="appendixA.html"><a href="appendixA.html#median"><i class="fa fa-check"></i><b>A.1.2</b> Median</a></li>
<li class="chapter" data-level="A.1.3" data-path="appendixA.html"><a href="appendixA.html#standard-deviation"><i class="fa fa-check"></i><b>A.1.3</b> Standard deviation</a></li>
<li class="chapter" data-level="A.1.4" data-path="appendixA.html"><a href="appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.4</b> Five-number summary</a></li>
<li class="chapter" data-level="A.1.5" data-path="appendixA.html"><a href="appendixA.html#distribution"><i class="fa fa-check"></i><b>A.1.5</b> Distribution</a></li>
<li class="chapter" data-level="A.1.6" data-path="appendixA.html"><a href="appendixA.html#outliers"><i class="fa fa-check"></i><b>A.1.6</b> Outliers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="example-4" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Example</h1>
<p>The following example was converted from PyTorch to rTorch to show differences and similarities of both approaches. The original source can be found here:</p>
<p>Source: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-tensors" class="uri">https://github.com/jcjohnson/pytorch-examples#pytorch-tensors</a></p>
<div id="load-the-libraries-1" class="section level2">
<h2><span class="header-section-number">8.1</span> Load the libraries</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">device =<span class="st"> </span>torch<span class="op">$</span><span class="kw">device</span>(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">torch<span class="op">$</span><span class="kw">manual_seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co">#&gt; &lt;torch._C.Generator&gt;</span></a></code></pre></div>
<ul>
<li><code>N</code> is batch size;</li>
<li><code>D_in</code> is input dimension;</li>
<li><code>H</code> is hidden dimension;</li>
<li><code>D_out</code> is output dimension.</li>
</ul>
</div>
<div id="dataset" class="section level2">
<h2><span class="header-section-number">8.2</span> Dataset</h2>
<p>We will create a random dataset for a <strong>two layer neural network</strong>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">N &lt;-<span class="st"> </span>64L; D_in &lt;-<span class="st"> </span>1000L; H &lt;-<span class="st"> </span>100L; D_out &lt;-<span class="st"> </span>10L</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># Create random Tensors to hold inputs and outputs</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">x &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_in, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">y &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_out, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="kw">dim</span>(x)</a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="co">#&gt; [1]   64 1000</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="kw">dim</span>(y)</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="co">#&gt; [1] 64 10</span></a></code></pre></div>
</div>
<div id="run-the-model-for-50-iterations" class="section level2">
<h2><span class="header-section-number">8.3</span> Run the model for 50 iterations</h2>
<p>Let’s say that for the sake of time we select to run only 50 iterations of the loop doing the training.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(D_in, H, <span class="dt">device=</span>device)   <span class="co"># layer 1</span></a>
<a class="sourceLine" id="cb3-3" data-line-number="3">w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(H, D_out, <span class="dt">device=</span>device)  <span class="co"># layer 2</span></a></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">learning_rate =<span class="st"> </span><span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co"># loop</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">  <span class="co"># Forward pass: compute predicted y, y_pred</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">  h &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">  h_relu &lt;-<span class="st"> </span>h<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">  y_pred &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb4-12" data-line-number="12">  loss &lt;-<span class="st"> </span>(torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">  <span class="co"># cat(t, &quot;\t&quot;)</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></a>
<a class="sourceLine" id="cb4-15" data-line-number="15"></a>
<a class="sourceLine" id="cb4-16" data-line-number="16">  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb4-17" data-line-number="17">  grad_y_pred &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">mul</span>(torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))</a>
<a class="sourceLine" id="cb4-18" data-line-number="18">  grad_w2 &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></a>
<a class="sourceLine" id="cb4-19" data-line-number="19">  grad_h_relu &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">mm</span>(w2<span class="op">$</span><span class="kw">t</span>())</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">  grad_h &lt;-<span class="st"> </span>grad_h_relu<span class="op">$</span><span class="kw">clone</span>()</a>
<a class="sourceLine" id="cb4-21" data-line-number="21">  mask &lt;-<span class="st"> </span>grad_h<span class="op">$</span><span class="kw">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></a>
<a class="sourceLine" id="cb4-22" data-line-number="22">  torch<span class="op">$</span><span class="kw">masked_select</span>(grad_h, mask)<span class="op">$</span><span class="kw">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></a>
<a class="sourceLine" id="cb4-23" data-line-number="23">  grad_w1 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></a>
<a class="sourceLine" id="cb4-24" data-line-number="24">   </a>
<a class="sourceLine" id="cb4-25" data-line-number="25">  <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb4-26" data-line-number="26">  w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w1, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w1))</a>
<a class="sourceLine" id="cb4-27" data-line-number="27">  w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w2, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w2))</a>
<a class="sourceLine" id="cb4-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb4-29" data-line-number="29"></a>
<a class="sourceLine" id="cb4-30" data-line-number="30">df_<span class="dv">50</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> y<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), </a>
<a class="sourceLine" id="cb4-31" data-line-number="31">                    <span class="dt">y_pred =</span> y_pred<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), <span class="dt">iter =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb4-32" data-line-number="32"></a>
<a class="sourceLine" id="cb4-33" data-line-number="33"><span class="kw">ggplot</span>(df_<span class="dv">50</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y, <span class="dt">y =</span> y_pred)) <span class="op">+</span></a>
<a class="sourceLine" id="cb4-34" data-line-number="34"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/run-model-1.png" width="70%" style="display: block; margin: auto;" />
We see a lot of dispersion between the predicted values, <span class="math inline">\(y_{pred}\)</span> and the real values, <span class="math inline">\(y\)</span>. We are far from our goal.</p>
</div>
<div id="run-it-at-100-iterations" class="section level2">
<h2><span class="header-section-number">8.4</span> Run it at 100 iterations</h2>
<p>Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of rthe algorithm.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">train &lt;-<span class="st"> </span><span class="cf">function</span>(iterations) {</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    <span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">    w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(D_in, H, <span class="dt">device=</span>device)   <span class="co"># layer 1</span></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(H, D_out, <span class="dt">device=</span>device)  <span class="co"># layer 2</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">    </a>
<a class="sourceLine" id="cb5-7" data-line-number="7">    learning_rate =<span class="st"> </span><span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb5-9" data-line-number="9">    <span class="co"># loop</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>iterations) {</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">      <span class="co"># Forward pass: compute predicted y</span></a>
<a class="sourceLine" id="cb5-12" data-line-number="12">      h &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">      h_relu &lt;-<span class="st"> </span>h<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb5-14" data-line-number="14">      y_pred &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">mm</span>(w2)</a>
<a class="sourceLine" id="cb5-15" data-line-number="15">    </a>
<a class="sourceLine" id="cb5-16" data-line-number="16">      <span class="co"># Compute and print loss; loss is a scalar stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb5-17" data-line-number="17">      <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb5-18" data-line-number="18">      loss &lt;-<span class="st"> </span>(torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</a>
<a class="sourceLine" id="cb5-19" data-line-number="19">      <span class="co"># cat(t, &quot;\t&quot;); cat(loss$item(), &quot;\n&quot;)</span></a>
<a class="sourceLine" id="cb5-20" data-line-number="20">    </a>
<a class="sourceLine" id="cb5-21" data-line-number="21">      <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb5-22" data-line-number="22">      grad_y_pred &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">mul</span>(torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))</a>
<a class="sourceLine" id="cb5-23" data-line-number="23">      grad_w2 &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_y_pred)</a>
<a class="sourceLine" id="cb5-24" data-line-number="24">      grad_h_relu &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">mm</span>(w2<span class="op">$</span><span class="kw">t</span>())</a>
<a class="sourceLine" id="cb5-25" data-line-number="25">      grad_h &lt;-<span class="st"> </span>grad_h_relu<span class="op">$</span><span class="kw">clone</span>()</a>
<a class="sourceLine" id="cb5-26" data-line-number="26">      mask &lt;-<span class="st"> </span>grad_h<span class="op">$</span><span class="kw">lt</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb5-27" data-line-number="27">      torch<span class="op">$</span><span class="kw">masked_select</span>(grad_h, mask)<span class="op">$</span><span class="kw">fill_</span>(<span class="fl">0.0</span>)</a>
<a class="sourceLine" id="cb5-28" data-line-number="28">      grad_w1 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_h)</a>
<a class="sourceLine" id="cb5-29" data-line-number="29">       </a>
<a class="sourceLine" id="cb5-30" data-line-number="30">      <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb5-31" data-line-number="31">      w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w1, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w1))</a>
<a class="sourceLine" id="cb5-32" data-line-number="32">      w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w2, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w2))</a>
<a class="sourceLine" id="cb5-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb5-34" data-line-number="34">    </a>
<a class="sourceLine" id="cb5-35" data-line-number="35">    <span class="kw">data.frame</span>(<span class="dt">y =</span> y<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), </a>
<a class="sourceLine" id="cb5-36" data-line-number="36">                        <span class="dt">y_pred =</span> y_pred<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), <span class="dt">iter =</span> iterations)</a>
<a class="sourceLine" id="cb5-37" data-line-number="37"></a>
<a class="sourceLine" id="cb5-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb5-39" data-line-number="39"></a>
<a class="sourceLine" id="cb5-40" data-line-number="40">df_<span class="dv">100</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb5-41" data-line-number="41"><span class="kw">ggplot</span>(df_<span class="dv">100</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb5-42" data-line-number="42"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/run-model-100-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Still there are differences between the value and the prediction. Let’s try with more iterations, like 250:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">df_<span class="dv">250</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">200</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="kw">ggplot</span>(df_<span class="dv">250</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias.</p>
<p>Let’s try one more time with 500 iterations:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">df_<span class="dv">500</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="kw">ggplot</span>(df_<span class="dv">500</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="original-pytorch-code" class="section level2">
<h2><span class="header-section-number">8.5</span> Original PyTorch code</h2>
<p>This code will not execute. It is shown here for reference. The running code will be written in <strong>rTorch</strong>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># Code in file tensor/two_layer_net_tensor.py</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb8-3" data-line-number="3"></a>
<a class="sourceLine" id="cb8-4" data-line-number="4">device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb8-6" data-line-number="6"></a>
<a class="sourceLine" id="cb8-7" data-line-number="7"><span class="co"># N is batch size; D_in is input dimension;</span></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co"># H is hidden dimension; D_out is output dimension.</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9">N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"></a>
<a class="sourceLine" id="cb8-11" data-line-number="11"><span class="co"># Create random input and output data</span></a>
<a class="sourceLine" id="cb8-12" data-line-number="12">x <span class="op">=</span> torch.randn(N, D_in, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb8-13" data-line-number="13">y <span class="op">=</span> torch.randn(N, D_out, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb8-14" data-line-number="14"></a>
<a class="sourceLine" id="cb8-15" data-line-number="15"><span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb8-16" data-line-number="16">w1 <span class="op">=</span> torch.randn(D_in, H, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb8-17" data-line-number="17">w2 <span class="op">=</span> torch.randn(H, D_out, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb8-18" data-line-number="18"></a>
<a class="sourceLine" id="cb8-19" data-line-number="19">learning_rate <span class="op">=</span> <span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb8-20" data-line-number="20"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</a>
<a class="sourceLine" id="cb8-21" data-line-number="21">  <span class="co"># Forward pass: compute predicted y</span></a>
<a class="sourceLine" id="cb8-22" data-line-number="22">  h <span class="op">=</span> x.mm(w1)</a>
<a class="sourceLine" id="cb8-23" data-line-number="23">  h_relu <span class="op">=</span> h.clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb8-24" data-line-number="24">  y_pred <span class="op">=</span> h_relu.mm(w2)</a>
<a class="sourceLine" id="cb8-25" data-line-number="25"></a>
<a class="sourceLine" id="cb8-26" data-line-number="26">  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb8-27" data-line-number="27">  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb8-28" data-line-number="28">  loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb8-29" data-line-number="29">  <span class="bu">print</span>(t, loss.item())</a>
<a class="sourceLine" id="cb8-30" data-line-number="30"></a>
<a class="sourceLine" id="cb8-31" data-line-number="31">  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb8-32" data-line-number="32">  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</a>
<a class="sourceLine" id="cb8-33" data-line-number="33">  grad_w2 <span class="op">=</span> h_relu.t().mm(grad_y_pred)</a>
<a class="sourceLine" id="cb8-34" data-line-number="34">  grad_h_relu <span class="op">=</span> grad_y_pred.mm(w2.t())</a>
<a class="sourceLine" id="cb8-35" data-line-number="35">  grad_h <span class="op">=</span> grad_h_relu.clone()</a>
<a class="sourceLine" id="cb8-36" data-line-number="36">  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb8-37" data-line-number="37">  grad_w1 <span class="op">=</span> x.t().mm(grad_h)</a>
<a class="sourceLine" id="cb8-38" data-line-number="38"></a>
<a class="sourceLine" id="cb8-39" data-line-number="39">  <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb8-40" data-line-number="40">  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</a>
<a class="sourceLine" id="cb8-41" data-line-number="41">  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2____</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rainfall-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-very-simple-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["rtorch-book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
